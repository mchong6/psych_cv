{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        dataset_type = type(dataset)\n",
    "        if dataset_type is torchvision.datasets.MNIST:\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
    "            return dataset.imgs[idx][1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "    \n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(val_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0\n",
    "\n",
    "exp = 'exp1'\n",
    "\n",
    "if exp == 'exp1':\n",
    "    num_classes = 14\n",
    "else:\n",
    "    num_classes = 13\n",
    "    \n",
    "num_epochs = 20\n",
    "classifier = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "classifier.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), 1e-3,\n",
    "#                             momentum=0.9,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    './data/%s'%exp,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=False, pin_memory=True, \n",
    "    sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "\n",
    "val_dataset = ImageFolderWithPaths('./test_data/%s'%exp, transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128, shuffle=False,\n",
    "    pin_memory=True, drop_last=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from PIL import Image\n",
    "# paths = glob.glob('./data/*/*/*', recursive=True)\n",
    "# # paths = glob.glob('./Google-Image-Downloader/fork/*', recursive=True)\n",
    "# failed = []\n",
    "# for path in paths:\n",
    "#     try:\n",
    "#         z = Image.open(path)\n",
    "#     except:\n",
    "#         failed.append(path)\n",
    "# print(failed)\n",
    "# for i in failed:\n",
    "#     try:\n",
    "#         os.remove(i)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][ 0/98]\tLoss 2.7016e+00 (2.7016e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  42.19 ( 42.19)\n",
      "Epoch: [0][10/98]\tLoss 1.3840e+00 (1.9085e+00)\tAcc@1  75.78 ( 54.19)\tAcc@5  97.66 ( 82.88)\n",
      "Epoch: [0][20/98]\tLoss 7.6161e-01 (1.4331e+00)\tAcc@1  82.81 ( 68.08)\tAcc@5  98.44 ( 90.10)\n",
      "Epoch: [0][30/98]\tLoss 5.2857e-01 (1.1606e+00)\tAcc@1  87.50 ( 74.45)\tAcc@5  98.44 ( 92.94)\n",
      "Epoch: [0][40/98]\tLoss 5.0018e-01 (1.0020e+00)\tAcc@1  88.28 ( 77.44)\tAcc@5  99.22 ( 94.42)\n",
      "Epoch: [0][50/98]\tLoss 4.5108e-01 (8.9473e-01)\tAcc@1  90.62 ( 79.47)\tAcc@5  98.44 ( 95.25)\n",
      "Epoch: [0][60/98]\tLoss 3.9874e-01 (8.1399e-01)\tAcc@1  85.16 ( 80.98)\tAcc@5  99.22 ( 95.84)\n",
      "Epoch: [0][70/98]\tLoss 3.6757e-01 (7.5274e-01)\tAcc@1  89.06 ( 82.13)\tAcc@5 100.00 ( 96.29)\n",
      "Epoch: [0][80/98]\tLoss 3.7467e-01 (7.0463e-01)\tAcc@1  88.28 ( 83.13)\tAcc@5  98.44 ( 96.60)\n",
      "Epoch: [0][90/98]\tLoss 4.4711e-01 (6.6793e-01)\tAcc@1  87.50 ( 83.83)\tAcc@5  96.09 ( 96.82)\n",
      "Test: [ 0/10]\tLoss 8.6590e-01 (8.6590e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  91.41 ( 91.41)\n",
      " * Acc@1 63.344 Acc@5 89.560\n",
      "Epoch: [1][ 0/98]\tLoss 3.3579e-01 (3.3579e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [1][10/98]\tLoss 3.1401e-01 (3.5611e-01)\tAcc@1  89.06 ( 89.70)\tAcc@5 100.00 ( 99.22)\n",
      "Epoch: [1][20/98]\tLoss 3.7000e-01 (3.6861e-01)\tAcc@1  92.19 ( 89.43)\tAcc@5  97.66 ( 98.92)\n",
      "Epoch: [1][30/98]\tLoss 4.7319e-01 (3.7490e-01)\tAcc@1  85.16 ( 89.11)\tAcc@5  98.44 ( 98.79)\n",
      "Epoch: [1][40/98]\tLoss 2.9402e-01 (3.6822e-01)\tAcc@1  89.84 ( 89.18)\tAcc@5  98.44 ( 98.78)\n",
      "Epoch: [1][50/98]\tLoss 3.0114e-01 (3.6708e-01)\tAcc@1  91.41 ( 89.22)\tAcc@5  98.44 ( 98.79)\n",
      "Epoch: [1][60/98]\tLoss 2.7208e-01 (3.6099e-01)\tAcc@1  92.19 ( 89.38)\tAcc@5 100.00 ( 98.87)\n",
      "Epoch: [1][70/98]\tLoss 3.3473e-01 (3.5882e-01)\tAcc@1  87.50 ( 89.43)\tAcc@5  99.22 ( 98.87)\n",
      "Epoch: [1][80/98]\tLoss 3.2767e-01 (3.5292e-01)\tAcc@1  89.06 ( 89.59)\tAcc@5  99.22 ( 98.89)\n",
      "Epoch: [1][90/98]\tLoss 3.2843e-01 (3.4597e-01)\tAcc@1  89.06 ( 89.75)\tAcc@5  99.22 ( 98.96)\n",
      "Test: [ 0/10]\tLoss 6.7419e-01 (6.7419e-01)\tAcc@1  84.38 ( 84.38)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 62.951 Acc@5 88.383\n",
      "Epoch: [2][ 0/98]\tLoss 4.1364e-01 (4.1364e-01)\tAcc@1  85.94 ( 85.94)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [2][10/98]\tLoss 2.5346e-01 (3.3131e-01)\tAcc@1  89.84 ( 90.41)\tAcc@5 100.00 ( 98.93)\n",
      "Epoch: [2][20/98]\tLoss 4.0799e-01 (3.1500e-01)\tAcc@1  83.59 ( 90.36)\tAcc@5 100.00 ( 99.14)\n",
      "Epoch: [2][30/98]\tLoss 2.9964e-01 (3.1364e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  98.44 ( 99.04)\n",
      "Epoch: [2][40/98]\tLoss 2.5869e-01 (3.1199e-01)\tAcc@1  89.84 ( 90.68)\tAcc@5 100.00 ( 99.05)\n",
      "Epoch: [2][50/98]\tLoss 3.3484e-01 (3.1019e-01)\tAcc@1  90.62 ( 90.76)\tAcc@5  99.22 ( 99.03)\n",
      "Epoch: [2][60/98]\tLoss 3.2095e-01 (3.1437e-01)\tAcc@1  85.94 ( 90.48)\tAcc@5  98.44 ( 99.00)\n",
      "Epoch: [2][70/98]\tLoss 2.7900e-01 (3.1893e-01)\tAcc@1  91.41 ( 90.35)\tAcc@5  99.22 ( 98.97)\n",
      "Epoch: [2][80/98]\tLoss 2.6270e-01 (3.1554e-01)\tAcc@1  91.41 ( 90.38)\tAcc@5 100.00 ( 99.04)\n",
      "Epoch: [2][90/98]\tLoss 3.9797e-01 (3.1276e-01)\tAcc@1  89.06 ( 90.50)\tAcc@5  98.44 ( 99.06)\n",
      "Test: [ 0/10]\tLoss 9.5988e-01 (9.5988e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  90.62 ( 90.62)\n",
      " * Acc@1 63.501 Acc@5 90.267\n",
      "Epoch: [3][ 0/98]\tLoss 1.8319e-01 (1.8319e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [3][10/98]\tLoss 2.6850e-01 (2.7594e-01)\tAcc@1  90.62 ( 91.19)\tAcc@5  99.22 ( 99.43)\n",
      "Epoch: [3][20/98]\tLoss 3.4695e-01 (3.1652e-01)\tAcc@1  85.16 ( 90.22)\tAcc@5  98.44 ( 99.07)\n",
      "Epoch: [3][30/98]\tLoss 3.7630e-01 (3.0935e-01)\tAcc@1  89.84 ( 90.30)\tAcc@5  99.22 ( 99.17)\n",
      "Epoch: [3][40/98]\tLoss 1.5310e-01 (2.9954e-01)\tAcc@1  96.09 ( 90.59)\tAcc@5 100.00 ( 99.26)\n",
      "Epoch: [3][50/98]\tLoss 2.5911e-01 (3.0378e-01)\tAcc@1  94.53 ( 90.56)\tAcc@5  99.22 ( 99.07)\n",
      "Epoch: [3][60/98]\tLoss 2.8690e-01 (3.0253e-01)\tAcc@1  89.06 ( 90.46)\tAcc@5  99.22 ( 99.09)\n",
      "Epoch: [3][70/98]\tLoss 3.3212e-01 (3.0463e-01)\tAcc@1  89.84 ( 90.42)\tAcc@5  97.66 ( 99.03)\n",
      "Epoch: [3][80/98]\tLoss 2.0140e-01 (3.0263e-01)\tAcc@1  96.09 ( 90.50)\tAcc@5  99.22 ( 99.02)\n",
      "Epoch: [3][90/98]\tLoss 3.2107e-01 (2.9800e-01)\tAcc@1  89.84 ( 90.66)\tAcc@5  99.22 ( 99.06)\n",
      "Test: [ 0/10]\tLoss 7.1486e-01 (7.1486e-01)\tAcc@1  82.81 ( 82.81)\tAcc@5  92.97 ( 92.97)\n",
      " * Acc@1 61.695 Acc@5 89.953\n",
      "Epoch: [4][ 0/98]\tLoss 2.1410e-01 (2.1410e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  98.44 ( 98.44)\n",
      "Epoch: [4][10/98]\tLoss 2.6093e-01 (3.0723e-01)\tAcc@1  89.84 ( 90.20)\tAcc@5 100.00 ( 99.22)\n",
      "Epoch: [4][20/98]\tLoss 2.0021e-01 (3.0440e-01)\tAcc@1  93.75 ( 90.33)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [4][30/98]\tLoss 2.4654e-01 (3.0374e-01)\tAcc@1  92.19 ( 90.62)\tAcc@5 100.00 ( 99.24)\n",
      "Epoch: [4][40/98]\tLoss 2.2152e-01 (2.9992e-01)\tAcc@1  94.53 ( 90.89)\tAcc@5  98.44 ( 99.26)\n",
      "Epoch: [4][50/98]\tLoss 2.1266e-01 (2.9298e-01)\tAcc@1  93.75 ( 91.12)\tAcc@5  99.22 ( 99.30)\n",
      "Epoch: [4][60/98]\tLoss 3.5583e-01 (2.9542e-01)\tAcc@1  87.50 ( 90.98)\tAcc@5  99.22 ( 99.30)\n",
      "Epoch: [4][70/98]\tLoss 2.7033e-01 (2.9580e-01)\tAcc@1  92.19 ( 91.02)\tAcc@5  97.66 ( 99.23)\n",
      "Epoch: [4][80/98]\tLoss 2.5557e-01 (2.9634e-01)\tAcc@1  92.97 ( 90.95)\tAcc@5  99.22 ( 99.23)\n",
      "Epoch: [4][90/98]\tLoss 2.6676e-01 (2.9935e-01)\tAcc@1  91.41 ( 90.85)\tAcc@5  99.22 ( 99.21)\n",
      "Test: [ 0/10]\tLoss 1.0463e+00 (1.0463e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  90.62 ( 90.62)\n",
      " * Acc@1 61.931 Acc@5 92.151\n",
      "Epoch: [5][ 0/98]\tLoss 3.9830e-01 (3.9830e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  97.66 ( 97.66)\n",
      "Epoch: [5][10/98]\tLoss 2.7450e-01 (3.2897e-01)\tAcc@1  89.84 ( 89.77)\tAcc@5  98.44 ( 98.58)\n",
      "Epoch: [5][20/98]\tLoss 1.9515e-01 (3.0592e-01)\tAcc@1  92.97 ( 90.81)\tAcc@5 100.00 ( 98.85)\n",
      "Epoch: [5][30/98]\tLoss 1.9254e-01 (2.8689e-01)\tAcc@1  92.97 ( 91.48)\tAcc@5  99.22 ( 99.02)\n",
      "Epoch: [5][40/98]\tLoss 2.7680e-01 (2.9133e-01)\tAcc@1  90.62 ( 91.14)\tAcc@5 100.00 ( 99.14)\n",
      "Epoch: [5][50/98]\tLoss 1.9682e-01 (2.8826e-01)\tAcc@1  93.75 ( 91.21)\tAcc@5 100.00 ( 99.13)\n",
      "Epoch: [5][60/98]\tLoss 2.3661e-01 (2.8939e-01)\tAcc@1  92.19 ( 91.20)\tAcc@5 100.00 ( 99.14)\n",
      "Epoch: [5][70/98]\tLoss 3.6102e-01 (2.9223e-01)\tAcc@1  89.06 ( 91.09)\tAcc@5 100.00 ( 99.13)\n",
      "Epoch: [5][80/98]\tLoss 3.9130e-01 (2.9313e-01)\tAcc@1  88.28 ( 90.99)\tAcc@5  98.44 ( 99.16)\n",
      "Epoch: [5][90/98]\tLoss 1.8850e-01 (2.8918e-01)\tAcc@1  93.75 ( 91.12)\tAcc@5 100.00 ( 99.18)\n",
      "Test: [ 0/10]\tLoss 1.1037e+00 (1.1037e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  89.84 ( 89.84)\n",
      " * Acc@1 61.695 Acc@5 89.011\n",
      "Epoch: [6][ 0/98]\tLoss 3.8968e-01 (3.8968e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  98.44 ( 98.44)\n",
      "Epoch: [6][10/98]\tLoss 1.6708e-01 (2.6788e-01)\tAcc@1  96.09 ( 92.05)\tAcc@5 100.00 ( 99.36)\n",
      "Epoch: [6][20/98]\tLoss 2.2216e-01 (2.7271e-01)\tAcc@1  91.41 ( 91.44)\tAcc@5 100.00 ( 99.33)\n",
      "Epoch: [6][30/98]\tLoss 2.1372e-01 (2.7001e-01)\tAcc@1  94.53 ( 91.71)\tAcc@5 100.00 ( 99.32)\n",
      "Epoch: [6][40/98]\tLoss 2.7240e-01 (2.6869e-01)\tAcc@1  90.62 ( 91.58)\tAcc@5  99.22 ( 99.33)\n",
      "Epoch: [6][50/98]\tLoss 1.4565e-01 (2.7348e-01)\tAcc@1  96.88 ( 91.47)\tAcc@5  99.22 ( 99.33)\n",
      "Epoch: [6][60/98]\tLoss 1.9629e-01 (2.7633e-01)\tAcc@1  93.75 ( 91.39)\tAcc@5 100.00 ( 99.33)\n",
      "Epoch: [6][70/98]\tLoss 3.7942e-01 (2.7844e-01)\tAcc@1  88.28 ( 91.37)\tAcc@5  98.44 ( 99.28)\n",
      "Epoch: [6][80/98]\tLoss 4.6175e-01 (2.8115e-01)\tAcc@1  85.94 ( 91.21)\tAcc@5  98.44 ( 99.27)\n",
      "Epoch: [6][90/98]\tLoss 2.1893e-01 (2.8085e-01)\tAcc@1  93.75 ( 91.29)\tAcc@5 100.00 ( 99.26)\n",
      "Test: [ 0/10]\tLoss 1.1399e+00 (1.1399e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  89.06 ( 89.06)\n",
      " * Acc@1 64.286 Acc@5 88.069\n",
      "Epoch: [7][ 0/98]\tLoss 3.1398e-01 (3.1398e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [7][10/98]\tLoss 4.1361e-01 (3.1938e-01)\tAcc@1  86.72 ( 89.63)\tAcc@5  98.44 ( 99.15)\n",
      "Epoch: [7][20/98]\tLoss 3.1833e-01 (2.8732e-01)\tAcc@1  89.84 ( 91.00)\tAcc@5  99.22 ( 99.11)\n",
      "Epoch: [7][30/98]\tLoss 3.6938e-01 (2.8804e-01)\tAcc@1  87.50 ( 90.83)\tAcc@5  97.66 ( 99.24)\n",
      "Epoch: [7][40/98]\tLoss 3.8651e-01 (2.9931e-01)\tAcc@1  89.06 ( 90.38)\tAcc@5  99.22 ( 99.24)\n",
      "Epoch: [7][50/98]\tLoss 3.8200e-01 (2.9322e-01)\tAcc@1  87.50 ( 90.50)\tAcc@5  98.44 ( 99.31)\n",
      "Epoch: [7][60/98]\tLoss 3.9150e-01 (2.8842e-01)\tAcc@1  87.50 ( 90.66)\tAcc@5  99.22 ( 99.33)\n",
      "Epoch: [7][70/98]\tLoss 3.6984e-01 (2.8871e-01)\tAcc@1  89.84 ( 90.70)\tAcc@5  99.22 ( 99.32)\n",
      "Epoch: [7][80/98]\tLoss 2.2187e-01 (2.8654e-01)\tAcc@1  92.97 ( 90.75)\tAcc@5 100.00 ( 99.35)\n",
      "Epoch: [7][90/98]\tLoss 2.8383e-01 (2.8512e-01)\tAcc@1  92.97 ( 90.83)\tAcc@5  99.22 ( 99.36)\n",
      "Test: [ 0/10]\tLoss 9.8098e-01 (9.8098e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  91.41 ( 91.41)\n",
      " * Acc@1 62.559 Acc@5 88.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][ 0/98]\tLoss 2.1208e-01 (2.1208e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [8][10/98]\tLoss 4.1439e-01 (2.7266e-01)\tAcc@1  87.50 ( 91.34)\tAcc@5  98.44 ( 99.50)\n",
      "Epoch: [8][20/98]\tLoss 2.1323e-01 (2.4939e-01)\tAcc@1  91.41 ( 92.22)\tAcc@5 100.00 ( 99.63)\n",
      "Epoch: [8][30/98]\tLoss 2.9323e-01 (2.5227e-01)\tAcc@1  90.62 ( 92.16)\tAcc@5  99.22 ( 99.42)\n",
      "Epoch: [8][40/98]\tLoss 2.0982e-01 (2.5922e-01)\tAcc@1  91.41 ( 91.62)\tAcc@5 100.00 ( 99.39)\n",
      "Epoch: [8][50/98]\tLoss 2.9723e-01 (2.5616e-01)\tAcc@1  89.06 ( 91.74)\tAcc@5  98.44 ( 99.45)\n",
      "Epoch: [8][60/98]\tLoss 2.1883e-01 (2.5796e-01)\tAcc@1  92.19 ( 91.66)\tAcc@5 100.00 ( 99.39)\n",
      "Epoch: [8][70/98]\tLoss 1.9968e-01 (2.5766e-01)\tAcc@1  91.41 ( 91.74)\tAcc@5  99.22 ( 99.37)\n",
      "Epoch: [8][80/98]\tLoss 1.8665e-01 (2.5168e-01)\tAcc@1  94.53 ( 91.88)\tAcc@5  99.22 ( 99.43)\n",
      "Epoch: [8][90/98]\tLoss 2.3687e-01 (2.5266e-01)\tAcc@1  92.97 ( 91.85)\tAcc@5  98.44 ( 99.40)\n",
      "Test: [ 0/10]\tLoss 8.5644e-01 (8.5644e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 60.126 Acc@5 89.325\n",
      "Epoch: [9][ 0/98]\tLoss 2.5902e-01 (2.5902e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [9][10/98]\tLoss 2.8255e-01 (2.7033e-01)\tAcc@1  91.41 ( 90.62)\tAcc@5  97.66 ( 99.22)\n",
      "Epoch: [9][20/98]\tLoss 2.0851e-01 (2.7202e-01)\tAcc@1  94.53 ( 91.07)\tAcc@5  99.22 ( 99.18)\n",
      "Epoch: [9][30/98]\tLoss 2.4620e-01 (2.7495e-01)\tAcc@1  91.41 ( 90.83)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [9][40/98]\tLoss 3.3163e-01 (2.8119e-01)\tAcc@1  89.84 ( 90.80)\tAcc@5  99.22 ( 99.26)\n",
      "Epoch: [9][50/98]\tLoss 2.8288e-01 (2.8002e-01)\tAcc@1  89.84 ( 90.99)\tAcc@5  99.22 ( 99.23)\n",
      "Epoch: [9][60/98]\tLoss 2.6497e-01 (2.8167e-01)\tAcc@1  90.62 ( 90.97)\tAcc@5 100.00 ( 99.23)\n",
      "Epoch: [9][70/98]\tLoss 2.3786e-01 (2.7813e-01)\tAcc@1  92.19 ( 91.09)\tAcc@5  99.22 ( 99.28)\n",
      "Epoch: [9][80/98]\tLoss 3.5125e-01 (2.7879e-01)\tAcc@1  92.19 ( 91.23)\tAcc@5  98.44 ( 99.26)\n",
      "Epoch: [9][90/98]\tLoss 3.5236e-01 (2.8045e-01)\tAcc@1  88.28 ( 91.12)\tAcc@5  99.22 ( 99.25)\n",
      "Test: [ 0/10]\tLoss 6.9358e-01 (6.9358e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  94.53 ( 94.53)\n",
      " * Acc@1 60.204 Acc@5 89.325\n",
      "Epoch: [10][ 0/98]\tLoss 2.2476e-01 (2.2476e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [10][10/98]\tLoss 1.0810e-01 (2.3033e-01)\tAcc@1  96.88 ( 93.04)\tAcc@5  99.22 ( 99.29)\n",
      "Epoch: [10][20/98]\tLoss 3.2916e-01 (2.2885e-01)\tAcc@1  90.62 ( 92.78)\tAcc@5  99.22 ( 99.33)\n",
      "Epoch: [10][30/98]\tLoss 1.9271e-01 (2.2225e-01)\tAcc@1  93.75 ( 93.02)\tAcc@5 100.00 ( 99.45)\n",
      "Epoch: [10][40/98]\tLoss 2.2352e-01 (2.2526e-01)\tAcc@1  93.75 ( 93.03)\tAcc@5  98.44 ( 99.47)\n",
      "Epoch: [10][50/98]\tLoss 3.8403e-01 (2.3348e-01)\tAcc@1  88.28 ( 92.62)\tAcc@5  98.44 ( 99.43)\n",
      "Epoch: [10][60/98]\tLoss 2.0404e-01 (2.3976e-01)\tAcc@1  90.62 ( 92.39)\tAcc@5 100.00 ( 99.40)\n",
      "Epoch: [10][70/98]\tLoss 2.7106e-01 (2.4385e-01)\tAcc@1  92.97 ( 92.29)\tAcc@5  98.44 ( 99.37)\n",
      "Epoch: [10][80/98]\tLoss 4.1822e-01 (2.4441e-01)\tAcc@1  85.94 ( 92.30)\tAcc@5  98.44 ( 99.34)\n",
      "Epoch: [10][90/98]\tLoss 2.4755e-01 (2.4627e-01)\tAcc@1  93.75 ( 92.26)\tAcc@5 100.00 ( 99.34)\n",
      "Test: [ 0/10]\tLoss 8.5991e-01 (8.5991e-01)\tAcc@1  78.12 ( 78.12)\tAcc@5  92.97 ( 92.97)\n",
      " * Acc@1 61.381 Acc@5 89.089\n",
      "Epoch: [11][ 0/98]\tLoss 2.0300e-01 (2.0300e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [11][10/98]\tLoss 2.1858e-01 (2.6533e-01)\tAcc@1  92.19 ( 91.34)\tAcc@5  99.22 ( 99.36)\n",
      "Epoch: [11][20/98]\tLoss 1.4117e-01 (2.4918e-01)\tAcc@1  96.09 ( 91.85)\tAcc@5 100.00 ( 99.48)\n",
      "Epoch: [11][30/98]\tLoss 2.4896e-01 (2.4502e-01)\tAcc@1  91.41 ( 92.14)\tAcc@5 100.00 ( 99.50)\n",
      "Epoch: [11][40/98]\tLoss 2.0596e-01 (2.4143e-01)\tAcc@1  93.75 ( 92.13)\tAcc@5 100.00 ( 99.52)\n",
      "Epoch: [11][50/98]\tLoss 1.9814e-01 (2.3869e-01)\tAcc@1  96.09 ( 92.16)\tAcc@5 100.00 ( 99.56)\n",
      "Epoch: [11][60/98]\tLoss 2.1024e-01 (2.3855e-01)\tAcc@1  94.53 ( 92.24)\tAcc@5 100.00 ( 99.54)\n",
      "Epoch: [11][70/98]\tLoss 2.6720e-01 (2.4482e-01)\tAcc@1  91.41 ( 91.97)\tAcc@5 100.00 ( 99.49)\n",
      "Epoch: [11][80/98]\tLoss 2.6020e-01 (2.4576e-01)\tAcc@1  91.41 ( 91.96)\tAcc@5  99.22 ( 99.49)\n",
      "Epoch: [11][90/98]\tLoss 2.3820e-01 (2.4492e-01)\tAcc@1  90.62 ( 91.96)\tAcc@5  99.22 ( 99.48)\n",
      "Test: [ 0/10]\tLoss 9.5966e-01 (9.5966e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 62.009 Acc@5 89.168\n",
      "Epoch: [12][ 0/98]\tLoss 3.0902e-01 (3.0902e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  98.44 ( 98.44)\n",
      "Epoch: [12][10/98]\tLoss 2.7326e-01 (2.3992e-01)\tAcc@1  90.62 ( 92.05)\tAcc@5 100.00 ( 99.50)\n",
      "Epoch: [12][20/98]\tLoss 1.5850e-01 (2.2820e-01)\tAcc@1  95.31 ( 92.45)\tAcc@5 100.00 ( 99.55)\n",
      "Epoch: [12][30/98]\tLoss 2.4300e-01 (2.3733e-01)\tAcc@1  94.53 ( 92.59)\tAcc@5  99.22 ( 99.40)\n",
      "Epoch: [12][40/98]\tLoss 2.3738e-01 (2.3418e-01)\tAcc@1  91.41 ( 92.53)\tAcc@5 100.00 ( 99.45)\n",
      "Epoch: [12][50/98]\tLoss 2.2614e-01 (2.2938e-01)\tAcc@1  94.53 ( 92.77)\tAcc@5  99.22 ( 99.46)\n",
      "Epoch: [12][60/98]\tLoss 2.7149e-01 (2.3026e-01)\tAcc@1  91.41 ( 92.76)\tAcc@5  99.22 ( 99.45)\n",
      "Epoch: [12][70/98]\tLoss 3.4274e-01 (2.3206e-01)\tAcc@1  92.19 ( 92.78)\tAcc@5  96.88 ( 99.36)\n",
      "Epoch: [12][80/98]\tLoss 2.1400e-01 (2.3605e-01)\tAcc@1  92.97 ( 92.70)\tAcc@5 100.00 ( 99.38)\n",
      "Epoch: [12][90/98]\tLoss 1.7732e-01 (2.3697e-01)\tAcc@1  92.97 ( 92.63)\tAcc@5 100.00 ( 99.37)\n",
      "Test: [ 0/10]\tLoss 9.8288e-01 (9.8288e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 62.873 Acc@5 90.267\n",
      "Epoch: [13][ 0/98]\tLoss 2.2473e-01 (2.2473e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [13][10/98]\tLoss 2.2255e-01 (2.3126e-01)\tAcc@1  93.75 ( 93.25)\tAcc@5 100.00 ( 99.50)\n",
      "Epoch: [13][20/98]\tLoss 2.4324e-01 (2.3733e-01)\tAcc@1  92.19 ( 93.01)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [13][30/98]\tLoss 1.7351e-01 (2.4499e-01)\tAcc@1  95.31 ( 92.49)\tAcc@5 100.00 ( 99.22)\n",
      "Epoch: [13][40/98]\tLoss 2.3736e-01 (2.4788e-01)\tAcc@1  92.97 ( 92.21)\tAcc@5 100.00 ( 99.33)\n",
      "Epoch: [13][50/98]\tLoss 3.8985e-01 (2.5078e-01)\tAcc@1  85.94 ( 91.99)\tAcc@5  98.44 ( 99.34)\n",
      "Epoch: [13][60/98]\tLoss 2.5052e-01 (2.5760e-01)\tAcc@1  90.62 ( 91.76)\tAcc@5 100.00 ( 99.33)\n",
      "Epoch: [13][70/98]\tLoss 3.0333e-01 (2.5951e-01)\tAcc@1  90.62 ( 91.74)\tAcc@5 100.00 ( 99.30)\n",
      "Epoch: [13][80/98]\tLoss 3.0184e-01 (2.6043e-01)\tAcc@1  89.06 ( 91.71)\tAcc@5  99.22 ( 99.31)\n",
      "Epoch: [13][90/98]\tLoss 1.6664e-01 (2.5634e-01)\tAcc@1  94.53 ( 91.90)\tAcc@5 100.00 ( 99.34)\n",
      "Test: [ 0/10]\tLoss 9.6892e-01 (9.6892e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 61.146 Acc@5 89.717\n",
      "Epoch: [14][ 0/98]\tLoss 1.4423e-01 (1.4423e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [14][10/98]\tLoss 3.5867e-01 (2.7288e-01)\tAcc@1  93.75 ( 91.83)\tAcc@5  99.22 ( 99.57)\n",
      "Epoch: [14][20/98]\tLoss 1.9831e-01 (2.4467e-01)\tAcc@1  94.53 ( 92.75)\tAcc@5  99.22 ( 99.48)\n",
      "Epoch: [14][30/98]\tLoss 2.3046e-01 (2.5491e-01)\tAcc@1  90.62 ( 92.46)\tAcc@5 100.00 ( 99.32)\n",
      "Epoch: [14][40/98]\tLoss 2.1350e-01 (2.5033e-01)\tAcc@1  95.31 ( 92.53)\tAcc@5 100.00 ( 99.31)\n",
      "Epoch: [14][50/98]\tLoss 1.2787e-01 (2.4320e-01)\tAcc@1  94.53 ( 92.63)\tAcc@5 100.00 ( 99.37)\n",
      "Epoch: [14][60/98]\tLoss 3.1614e-01 (2.4480e-01)\tAcc@1  88.28 ( 92.57)\tAcc@5  99.22 ( 99.36)\n",
      "Epoch: [14][70/98]\tLoss 3.0663e-01 (2.4759e-01)\tAcc@1  89.06 ( 92.35)\tAcc@5  98.44 ( 99.38)\n",
      "Epoch: [14][80/98]\tLoss 2.7151e-01 (2.4589e-01)\tAcc@1  92.97 ( 92.48)\tAcc@5 100.00 ( 99.39)\n",
      "Epoch: [14][90/98]\tLoss 1.8945e-01 (2.5022e-01)\tAcc@1  94.53 ( 92.35)\tAcc@5 100.00 ( 99.38)\n",
      "Test: [ 0/10]\tLoss 7.5444e-01 (7.5444e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  92.97 ( 92.97)\n",
      " * Acc@1 61.852 Acc@5 89.874\n",
      "Epoch: [15][ 0/98]\tLoss 2.3766e-01 (2.3766e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [15][10/98]\tLoss 2.8730e-01 (2.3615e-01)\tAcc@1  87.50 ( 91.62)\tAcc@5  99.22 ( 99.64)\n",
      "Epoch: [15][20/98]\tLoss 1.4608e-01 (2.3561e-01)\tAcc@1  94.53 ( 91.52)\tAcc@5 100.00 ( 99.52)\n",
      "Epoch: [15][30/98]\tLoss 2.1960e-01 (2.3392e-01)\tAcc@1  93.75 ( 91.81)\tAcc@5 100.00 ( 99.52)\n",
      "Epoch: [15][40/98]\tLoss 2.0825e-01 (2.3265e-01)\tAcc@1  89.84 ( 91.90)\tAcc@5 100.00 ( 99.50)\n",
      "Epoch: [15][50/98]\tLoss 3.3081e-01 (2.3452e-01)\tAcc@1  89.84 ( 91.94)\tAcc@5  99.22 ( 99.51)\n",
      "Epoch: [15][60/98]\tLoss 3.0569e-01 (2.3535e-01)\tAcc@1  92.97 ( 92.07)\tAcc@5  97.66 ( 99.53)\n",
      "Epoch: [15][70/98]\tLoss 2.3700e-01 (2.3732e-01)\tAcc@1  91.41 ( 92.08)\tAcc@5  99.22 ( 99.46)\n",
      "Epoch: [15][80/98]\tLoss 1.6232e-01 (2.3636e-01)\tAcc@1  96.09 ( 92.21)\tAcc@5 100.00 ( 99.49)\n",
      "Epoch: [15][90/98]\tLoss 2.3639e-01 (2.3807e-01)\tAcc@1  92.19 ( 92.17)\tAcc@5 100.00 ( 99.48)\n",
      "Test: [ 0/10]\tLoss 9.2479e-01 (9.2479e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 61.695 Acc@5 89.717\n",
      "Epoch: [16][ 0/98]\tLoss 1.7378e-01 (1.7378e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [16][10/98]\tLoss 1.9785e-01 (2.4838e-01)\tAcc@1  92.19 ( 92.54)\tAcc@5 100.00 ( 99.72)\n",
      "Epoch: [16][20/98]\tLoss 1.7631e-01 (2.3495e-01)\tAcc@1  95.31 ( 92.75)\tAcc@5  98.44 ( 99.70)\n",
      "Epoch: [16][30/98]\tLoss 2.5805e-01 (2.2893e-01)\tAcc@1  90.62 ( 92.79)\tAcc@5  98.44 ( 99.60)\n",
      "Epoch: [16][40/98]\tLoss 2.7641e-01 (2.3074e-01)\tAcc@1  88.28 ( 92.61)\tAcc@5 100.00 ( 99.54)\n",
      "Epoch: [16][50/98]\tLoss 2.4916e-01 (2.3119e-01)\tAcc@1  90.62 ( 92.68)\tAcc@5  99.22 ( 99.49)\n",
      "Epoch: [16][60/98]\tLoss 2.6526e-01 (2.3350e-01)\tAcc@1  90.62 ( 92.43)\tAcc@5  98.44 ( 99.51)\n",
      "Epoch: [16][70/98]\tLoss 2.9491e-01 (2.3841e-01)\tAcc@1  90.62 ( 92.39)\tAcc@5 100.00 ( 99.54)\n",
      "Epoch: [16][80/98]\tLoss 2.2764e-01 (2.3815e-01)\tAcc@1  92.97 ( 92.40)\tAcc@5 100.00 ( 99.55)\n",
      "Epoch: [16][90/98]\tLoss 3.1011e-01 (2.4026e-01)\tAcc@1  89.84 ( 92.38)\tAcc@5  99.22 ( 99.51)\n",
      "Test: [ 0/10]\tLoss 9.0149e-01 (9.0149e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  92.97 ( 92.97)\n",
      " * Acc@1 61.460 Acc@5 89.403\n",
      "Epoch: [17][ 0/98]\tLoss 3.5169e-01 (3.5169e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [17][10/98]\tLoss 2.0624e-01 (2.6196e-01)\tAcc@1  92.97 ( 91.76)\tAcc@5 100.00 ( 99.43)\n",
      "Epoch: [17][20/98]\tLoss 1.6153e-01 (2.5721e-01)\tAcc@1  95.31 ( 91.74)\tAcc@5  99.22 ( 99.52)\n",
      "Epoch: [17][30/98]\tLoss 2.5596e-01 (2.4711e-01)\tAcc@1  90.62 ( 92.14)\tAcc@5  99.22 ( 99.50)\n",
      "Epoch: [17][40/98]\tLoss 1.4288e-01 (2.4472e-01)\tAcc@1  92.97 ( 92.24)\tAcc@5  99.22 ( 99.45)\n",
      "Epoch: [17][50/98]\tLoss 2.7321e-01 (2.4764e-01)\tAcc@1  92.97 ( 92.16)\tAcc@5  99.22 ( 99.43)\n",
      "Epoch: [17][60/98]\tLoss 1.7854e-01 (2.4674e-01)\tAcc@1  92.97 ( 92.15)\tAcc@5 100.00 ( 99.46)\n",
      "Epoch: [17][70/98]\tLoss 2.5490e-01 (2.4781e-01)\tAcc@1  89.84 ( 92.11)\tAcc@5  99.22 ( 99.45)\n",
      "Epoch: [17][80/98]\tLoss 1.5556e-01 (2.4919e-01)\tAcc@1  96.09 ( 92.05)\tAcc@5 100.00 ( 99.41)\n",
      "Epoch: [17][90/98]\tLoss 2.2722e-01 (2.4648e-01)\tAcc@1  92.97 ( 92.17)\tAcc@5 100.00 ( 99.43)\n",
      "Test: [ 0/10]\tLoss 9.1313e-01 (9.1313e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 62.088 Acc@5 89.874\n",
      "Epoch: [18][ 0/98]\tLoss 2.3399e-01 (2.3399e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [18][10/98]\tLoss 3.1143e-01 (2.6759e-01)\tAcc@1  91.41 ( 92.12)\tAcc@5 100.00 ( 99.08)\n",
      "Epoch: [18][20/98]\tLoss 2.7053e-01 (2.6426e-01)\tAcc@1  89.06 ( 91.70)\tAcc@5  99.22 ( 99.18)\n",
      "Epoch: [18][30/98]\tLoss 1.7053e-01 (2.5647e-01)\tAcc@1  92.97 ( 91.76)\tAcc@5 100.00 ( 99.22)\n",
      "Epoch: [18][40/98]\tLoss 2.4739e-01 (2.5174e-01)\tAcc@1  89.84 ( 92.09)\tAcc@5 100.00 ( 99.28)\n",
      "Epoch: [18][50/98]\tLoss 2.0506e-01 (2.4669e-01)\tAcc@1  92.97 ( 92.03)\tAcc@5 100.00 ( 99.36)\n",
      "Epoch: [18][60/98]\tLoss 3.3565e-01 (2.5282e-01)\tAcc@1  91.41 ( 91.88)\tAcc@5 100.00 ( 99.39)\n",
      "Epoch: [18][70/98]\tLoss 2.7723e-01 (2.4682e-01)\tAcc@1  87.50 ( 92.01)\tAcc@5 100.00 ( 99.46)\n",
      "Epoch: [18][80/98]\tLoss 2.7498e-01 (2.4678e-01)\tAcc@1  90.62 ( 92.07)\tAcc@5  98.44 ( 99.43)\n",
      "Epoch: [18][90/98]\tLoss 3.6817e-01 (2.5004e-01)\tAcc@1  86.72 ( 91.95)\tAcc@5  99.22 ( 99.45)\n",
      "Test: [ 0/10]\tLoss 9.8618e-01 (9.8618e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 61.460 Acc@5 89.874\n",
      "Epoch: [19][ 0/98]\tLoss 1.9206e-01 (1.9206e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [19][10/98]\tLoss 2.0013e-01 (2.2219e-01)\tAcc@1  92.97 ( 93.11)\tAcc@5 100.00 ( 99.72)\n",
      "Epoch: [19][20/98]\tLoss 2.4074e-01 (2.2270e-01)\tAcc@1  91.41 ( 92.89)\tAcc@5 100.00 ( 99.59)\n",
      "Epoch: [19][30/98]\tLoss 1.7831e-01 (2.2010e-01)\tAcc@1  92.19 ( 92.89)\tAcc@5 100.00 ( 99.50)\n",
      "Epoch: [19][40/98]\tLoss 3.4067e-01 (2.2588e-01)\tAcc@1  88.28 ( 92.72)\tAcc@5  98.44 ( 99.45)\n",
      "Epoch: [19][50/98]\tLoss 1.6425e-01 (2.2863e-01)\tAcc@1  95.31 ( 92.75)\tAcc@5 100.00 ( 99.46)\n",
      "Epoch: [19][60/98]\tLoss 3.8490e-01 (2.3526e-01)\tAcc@1  89.84 ( 92.64)\tAcc@5 100.00 ( 99.44)\n",
      "Epoch: [19][70/98]\tLoss 2.6344e-01 (2.3483e-01)\tAcc@1  92.97 ( 92.76)\tAcc@5  98.44 ( 99.44)\n",
      "Epoch: [19][80/98]\tLoss 2.6931e-01 (2.3438e-01)\tAcc@1  91.41 ( 92.70)\tAcc@5  99.22 ( 99.48)\n",
      "Epoch: [19][90/98]\tLoss 2.3812e-01 (2.3341e-01)\tAcc@1  89.84 ( 92.65)\tAcc@5 100.00 ( 99.47)\n",
      "Test: [ 0/10]\tLoss 9.2082e-01 (9.2082e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 62.873 Acc@5 89.560\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "    \n",
    "    if epoch == num_epochs // 2:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] /= 10\n",
    "\n",
    "\n",
    "    for i, (im, target) in enumerate(train_loader):\n",
    "        classifier.train()\n",
    "        im = im.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        output = classifier(im)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), im.size(0))\n",
    "        top1.update(acc1[0], im.size(0))\n",
    "        top5.update(acc5[0], im.size(0))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "    acc1 = validate(val_loader, classifier, criterion)\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "    if is_best:\n",
    "        torch.save(classifier.state_dict(), 'model_%s.pth'%exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 64.286 Acc@5 88.069\n"
     ]
    }
   ],
   "source": [
    "out_dict = {}\n",
    "idx2class = train_dataset.classes\n",
    "classifier.load_state_dict(torch.load('model_%s.pth'%exp))\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, target, path) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = classifier(images)\n",
    "        val, idx = F.softmax(output, 1).sort(dim=1, descending=True)  # [B, num_classes]\n",
    "        for i in range(idx.size(0)):\n",
    "            labels = [\"%s: %.3f\" % (idx2class[idx[i][j]], val[i][j].item()) for j in range(num_classes)]\n",
    "            labels = \"Actual: %s, \"%idx2class[target[i]] + \", \".join(labels)\n",
    "            out_dict[path[i]] = labels\n",
    "\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "    # TODO: this should also be done with the ProgressMeter\n",
    "    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "print(len(out_dict))\n",
    "\n",
    "w = csv.writer(open(\"nocrop_output_%s.csv\"%exp, \"w\"))\n",
    "for key, val in out_dict.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# in_dir = './Images/coco_images'\n",
    "# out_dir = './exp2/test_data'\n",
    "\n",
    "# idx2class  = {1:'person',3:'car',5:'airplane',16:'bird',18:'dog',22:'elephant',23:'bear',24:'zebra',48:'fork',\n",
    "#               49:'knife',51:'bowl',53:'apple',62:'chair',67:'table'}\n",
    " \n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "# for k,v in idx2class.items():\n",
    "#     out = os.path.join(out_dir, v)\n",
    "#     os.makedirs(out, exist_ok=True)\n",
    "\n",
    "\n",
    "# csv_dict = {}\n",
    "# f =  open(\"Book1.csv\")\n",
    "# lis = [line.strip().split(',') for line in f]        # create a list of lists\n",
    "# headers = lis.pop(0)   \n",
    "# for i, header in enumerate(headers):\n",
    "#     csv_dict[header] = i\n",
    "\n",
    "# for item in lis:\n",
    "#     name = item[csv_dict['filename']][1:-1]\n",
    "#     try:\n",
    "#         category = int(item[csv_dict['category_id']])\n",
    "#     except:\n",
    "#         category = item[csv_dict['category_id']]\n",
    "#         break\n",
    "#     input_name = os.path.join(in_dir, name)\n",
    "#     output_name = os.path.join(out_dir, idx2class[category], name)\n",
    "#     try:\n",
    "#         shutil.move(input_name, output_name)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
